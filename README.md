# mcp_server_openai

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastMCP](https://img.shields.io/badge/FastMCP-compatible-green.svg)](https://github.com/jlowin/fastmcp)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A production-ready FastMCP server implementation using the official MCP SDK. Provides mathematical operations, web content fetching, PowerPoint generation, and intelligent prompt management with client-specific configurations.

## âœ¨ Features

- **ğŸ”§ Tools**: Mathematical operations, web fetching, PowerPoint content generation
- **ğŸ“Š Resources**: Health monitoring and system status
- **ğŸ¯ Prompts**: Jinja2-based templates with per-client customization
- **ğŸš€ Dual Interface**: Both stdio and HTTP/SSE modes
- **ğŸ“ Structured Logging**: JSON-formatted request lifecycle tracking
- **ğŸ“ˆ Progress Tracking**: Real-time progress monitoring with ETA calculation and hierarchical support
- **ğŸ’° Usage Monitoring**: Comprehensive Claude API cost tracking and rate limiting
- **ğŸ”„ Real-time Streaming**: Enhanced SSE/WebSocket with live usage updates
- **ğŸ³ Docker Ready**: Containerized deployment support

---

## ğŸ“‹ Table of Contents

- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“¦ Requirements](#-requirements)
- [ğŸ’¾ Installation](#-installation)
- [ğŸ® Usage](#-usage)
- [ğŸ“– API Reference](#-api-reference)
- [ğŸ“ˆ Progress Tracking](#-progress-tracking)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ§ª Testing](#-testing)
- [ğŸ³ Docker](#-docker)
- [ğŸ› ï¸ Development](#ï¸-development)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ“ˆ Changelog](#-changelog)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

---

## ğŸš€ Quick Start

1.  **Install Dependencies:**

    ```bash
    # Install with uv (recommended)
    uv venv && uv sync
    ```

2.  **Run the Server:**

    *   **Stdio Mode (for development):**

        ```bash
        uv run mcp dev src/mcp_server_openai/server.py:app
        ```

    *   **HTTP Mode (for production):**

        ```bash
        uv run uvicorn mcp_server_openai.streaming_http:app --host 0.0.0.0 --port 8000
        ```

---

## ğŸ“¦ Requirements

- **Python 3.10+**
- **Node.js LTS** (for MCP Inspector; provides `npx`)
- **Optional**: Docker for containerized deployment
- **Recommended**: [`uv`](https://github.com/astral-sh/uv) for fast, reproducible environments

---

## ğŸ’¾ Installation

### Using pip

```bash
python -m pip install -U pip
pip install -e .
```

### Using uv (Recommended)

```bash
uv venv
uv sync
```

---

## ğŸ® Usage

### Stdio Mode with MCP Dev Inspector

The stdio mode is ideal for development and debugging.

```bash
# Install the MCP CLI if you haven't already
uv add "mcp[cli]"

# Start the inspector and spawn the server over stdio
uv run mcp dev src/mcp_server_openai/server.py:app
```

#### Windows Setup

Ensure `npx` is available on your `PATH`. Common locations include:
- `C:\Program Files\nodejs\npx.cmd`
- `%USERPROFILE%\AppData\Roaming\npm\npx.cmd`

### HTTP Mode with Modern Streaming

The HTTP mode provides a robust, production-ready server with advanced features.

```bash
# Start the modern HTTP server
uv run uvicorn mcp_server_openai.streaming_http:app --host 0.0.0.0 --port 8000

# Or use the enhanced server runner for graceful shutdown
uv run python -m mcp_server_openai.enhanced_server --host 0.0.0.0 --port 8000
```

You can interact with the server using `curl` or any HTTP client.

---

## ğŸ“– API Reference

### ğŸ”§ Tools

#### `math.add` / `math.sub`

Performs basic mathematical operations.

- **Parameters:** `{"a": <number>, "b": <number>}`
- **Returns:** The result of the operation.

#### `web.fetch_url`

Fetches web content and returns a comprehensive set of metadata.

- **Parameters:** `{"url": "<string>"}`
- **Returns:** A JSON object with the URL, status code, headers, and content preview.

#### `content.create`

Generates a PowerPoint presentation from structured data.

- **Parameters:** A JSON object with details about the presentation.
- **Output:** A `.pptx` file saved to the `output/<Client>/<Project>/` directory.

### ğŸ“Š Resources

#### `health://ping`

A health check endpoint that returns the server's status.

### ğŸ¯ Prompts

The server includes an enhanced prompt management system with advanced template capabilities.

#### Available Prompts

- **`summarize`**: An advanced summarization prompt with client-specific customization.
- **`content_create`**: A template-based content creation system with rich customization.

---

## ğŸ“ˆ Progress Tracking

The server includes a modern progress tracking system that provides real-time monitoring, percentage tracking, ETA calculation, and hierarchical progress support.

--- 

## âš™ï¸ Configuration

### Per-Client Prompt Variables

You can configure client-specific prompt behavior using either a YAML file or a JSON environment variable. Note that these options are mutually exclusive.

#### YAML Configuration

Create a `config.yaml` file and set the `MCP_CONFIG_PATH` environment variable to its path.

```yaml
prompts:
  summarize:
    defaults:
      tone: concise
    clients:
      acme:
        tone: detailed
        style: professional
```

#### JSON Environment Variable

Set the `MCP_CONFIG_JSON` environment variable to a JSON string.

```bash
export MCP_CONFIG_JSON='{"prompts":{"summarize":{"defaults":{"tone":"concise"}},"clients":{"acme":{"tone":"detailed"}}}}}'
```

--- 

## ğŸ§ª Testing

To run the test suite, use the following command:

```bash
# Using uv (recommended)
uv run python -m pytest -q

# Using pytest directly
pytest
```

After making any changes to the code, it is important to run the tests to ensure that everything is still working correctly.

--- 

## ğŸ³ Docker

You can build and run the server in a Docker container.

```bash
# Build the image
docker build -t mcp-server-openai:0.2.0 .

# Run the container
docker run --rm -p 8000:8000 mcp-server-openai:0.2.0
```

--- 

## ğŸ› ï¸ Development

This project uses `make` to streamline common development tasks.

```bash
make check       # Run all checks
make fmt         # Format code
make lint        # Lint code
make test        # Run tests
```

--- 

## ğŸ“ Project Structure

```
src/mcp_server_openai/
â”œâ”€â”€ __main__.py              # CLI entrypoint
â”œâ”€â”€ main.py                  # Main application logic
â”œâ”€â”€ server.py                # FastMCP app factory + auto-discovery
â”œâ”€â”€ http_server.py           # Legacy HTTP/SSE server (Starlette)
â”œâ”€â”€ streaming_http.py        # Modern streamable HTTP server with advanced features
â”œâ”€â”€ enhanced_server.py       # Enhanced server runner with graceful shutdown
â”œâ”€â”€ server_config.py         # Configuration management system
â”œâ”€â”€ error_handling.py        # Comprehensive error handling and monitoring
â”œâ”€â”€ config.py                # YAML/JSON config loader
â”œâ”€â”€ logging_utils.py         # Structured JSON logging
â”œâ”€â”€ progress.py              # Modern progress tracking with ETA & hierarchical support
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ math_tools.py        # Mathematical operations
â”‚   â”œâ”€â”€ web_tools.py         # Web content fetching
â”‚   â””â”€â”€ content_creator.py   # PowerPoint generation
â”œâ”€â”€ resources/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ health.py            # Health check resources
â””â”€â”€ prompts/
    â”œâ”€â”€ __init__.py          # Enhanced prompt system exports
    â”œâ”€â”€ manager.py           # Modern prompt manager with advanced features
    â”œâ”€â”€ summarize.py         # Summarization prompts
    â”œâ”€â”€ content_create.py    # Content creation prompts
    â”œâ”€â”€ README.md           # Prompt system documentation
    â””â”€â”€ templates/
        â”œâ”€â”€ base.j2         # Base template with inheritance
        â”œâ”€â”€ summarize.j2    # Summarization template
        â””â”€â”€ content_create.j2 # Content creation template

scripts/
â”œâ”€â”€ call_tool.py             # CLI tool caller
â”œâ”€â”€ cli.sh                   # Shell utilities
â”œâ”€â”€ preflight.py             # Code quality checks
â””â”€â”€ run.sh                   # Runtime scripts

tests/                       # Comprehensive test suite
â”œâ”€â”€ conftest.py
â”œâ”€â”€ test_*.py               # Unit tests for all modules
â”œâ”€â”€ test_streaming_http.py  # Tests for modern streaming features
â””â”€â”€ ...
```

--- 

## ğŸ“ˆ Changelog

### v0.2.0 (Current)

- **Milestone 2: Registry & Prompts**
  - Implemented auto-discovery of tools.
  - Added a YAML/JSON config loader with per-client prompt variables.
  - Integrated Jinja2-based prompt templates with client overrides.
  - Implemented structured JSON logging with request lifecycle tracking.
- **Enhanced Prompt Management System v2.0**
  - Developed a modern, robust prompt management system.
  - Added template validation and health checks.
  - Implemented advanced caching with TTL and invalidation strategies.
- **Milestone 3.1: HTTP/SSE**
  - Implemented Server-Sent Events streaming with keep-alive.
  - Added health and info endpoints for monitoring.
- **Milestone 3.2: Progress Tracking**
  - Implemented a modern progress tracking system with percentage & ETA calculation.
  - Added hierarchical progress support for complex workflows.
- **Milestone 3.3: Modern Streamable HTTP**
  - Added HTTP/2 support with ALPN negotiation.
  - Implemented enhanced Server-Sent Events with multiplexing and capability negotiation.
  - Integrated WebSocket for real-time bidirectional communication.
- **Code Quality & Type Safety Improvements**
  - Implemented comprehensive mypy type checking with strict compliance.
  - Enhanced type annotations across all modules.

--- 

## ğŸ¤ Contributing

1.  Fork the repository.
2.  Create a feature branch: `git checkout -b feature-name`
3.  Make your changes, following the existing code style.
4.  Run the quality checks: `make check`
5.  Submit a pull request.

--- 

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

--- 

*Built with â¤ï¸ using FastMCP and the Model Context Protocol*